{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:30:22.868559Z",
     "end_time": "2023-05-01T13:31:04.485626Z"
    },
    "id": "QbKdQimXiXwn"
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch -y &> /dev/null\n",
    "!pip install torch==1.13.1 &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:04.102608Z",
     "end_time": "2023-05-01T13:47:04.117966Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:04.369148Z",
     "end_time": "2023-05-01T13:47:04.380975Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:04.735096Z",
     "end_time": "2023-05-01T13:47:04.746618Z"
    },
    "id": "QJdniX0Iidds"
   },
   "outputs": [],
   "source": [
    "class MlpNet(nn.Module):\n",
    "    def __init__(self, width_ratio=-1):\n",
    "        super(MlpNet, self).__init__()\n",
    "        input_dim = 784 # [mnist] 28 x 28 x 1\n",
    "        if width_ratio != -1:\n",
    "            self.width_ratio = width_ratio\n",
    "        else:\n",
    "            self.width_ratio = 1\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, int(400/self.width_ratio), bias=not True)\n",
    "        self.fc2 = nn.Linear(int(400/self.width_ratio), int(200/self.width_ratio), bias=not True)\n",
    "        self.fc3 = nn.Linear(int(200/self.width_ratio), int(100/self.width_ratio), bias=not True)\n",
    "        self.fc4 = nn.Linear(int(100/self.width_ratio), 10, bias=not True)\n",
    "        self.enable_dropout = False\n",
    "\n",
    "    def forward(self, x, disable_logits=True):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.enable_dropout:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if self.enable_dropout:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        if self.enable_dropout:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        if disable_logits:\n",
    "            return x\n",
    "        else:\n",
    "            return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:05.404754Z",
     "end_time": "2023-05-01T13:47:05.716440Z"
    },
    "id": "dza4_FenZ7wm"
   },
   "outputs": [],
   "source": [
    "# Define the transform to apply to the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the MNIST dataset, only keeping images of number 4\n",
    "mnist = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:05.953925Z",
     "end_time": "2023-05-01T13:47:05.971838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.targets.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:06.876737Z",
     "end_time": "2023-05-01T13:47:06.904274Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_not_fours = np.where(np.array(mnist.targets) != 4)[0]\n",
    "dataset_not_fours = torch.utils.data.Subset(mnist, idx_not_fours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:07.765453Z",
     "end_time": "2023-05-01T13:47:07.775922Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.DataLoader(dataset_not_fours, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:08.281601Z",
     "end_time": "2023-05-01T13:47:08.936569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset.dataset[81][0].numpy().squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:11.426690Z",
     "end_time": "2023-05-01T13:47:11.433694Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:47:12.581461Z",
     "end_time": "2023-05-01T13:47:12.647639Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the split ratio\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "# calculate the sizes of the training and validation sets\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# use random_split to split the Subset into train and validation subsets\n",
    "training_data, test_data = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "lr:  0.0001\n",
      "momentum:  0.9\n",
      "batch_size:  64\n",
      "epochs:  10\n",
      "weight_decay:  0.001\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# HYPERPARAMETERS\n",
    "lr = 0.0001\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "weight_decay = 0.001\n",
    "# --------------\n",
    "#print the hyperparameters in a table\n",
    "print(\"Hyperparameters:\")\n",
    "print(\"lr: \",lr)\n",
    "print(\"momentum: \",momentum)\n",
    "print(\"batch_size: \",batch_size)\n",
    "print(\"epochs: \",epochs)\n",
    "print(\"weight_decay: \",weight_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T13:50:43.441429Z",
     "end_time": "2023-05-01T13:50:43.448352Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:50:43.552310Z",
     "end_time": "2023-05-01T13:50:43.569397Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmgCLKM-hlA2",
    "outputId": "72394a8c-9ca0-4bf9-8393-89e2cfe952f8"
   },
   "outputs": [],
   "source": [
    "model = MlpNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:50:43.699725Z",
     "end_time": "2023-05-01T13:50:43.710068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The traning set has  43326 images.\n",
      "The test set has  10832 images.\n",
      "The batch size is  64 .\n",
      "There will be 676 batches per epoch.\n"
     ]
    }
   ],
   "source": [
    "print(\"The traning set has \",len(training_data), \"images.\")\n",
    "print(\"The test set has \",len(test_data), \"images.\")\n",
    "print(\"The batch size is \",batch_size, \".\")\n",
    "print(\"There will be\",int(len(training_data)/batch_size), \"batches per epoch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T13:50:44.070012Z",
     "end_time": "2023-05-01T13:50:44.077579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of train_dataloader: 677\n",
      "Lenght of test_dataloader: 170\n",
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Lenght of train_dataloader: {len(train_dataloader)}\")\n",
    "print(f\"Lenght of test_dataloader: {len(test_dataloader)}\")\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T13:50:44.323187Z",
     "end_time": "2023-05-01T13:50:44.673687Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:50:45.350692Z",
     "end_time": "2023-05-01T13:50:45.361626Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, plot_the_loss=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    losses = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>4f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    if plot_the_loss:\n",
    "        plt.plot(losses)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "loss: 2.168167  [   64/43326]\n",
      "loss: 2.175913  [  704/43326]\n",
      "loss: 2.184946  [ 1344/43326]\n",
      "loss: 2.194246  [ 1984/43326]\n",
      "loss: 2.169518  [ 2624/43326]\n",
      "loss: 2.180944  [ 3264/43326]\n",
      "loss: 2.165131  [ 3904/43326]\n",
      "loss: 2.172295  [ 4544/43326]\n",
      "loss: 2.170720  [ 5184/43326]\n",
      "loss: 2.164687  [ 5824/43326]\n",
      "loss: 2.153788  [ 6464/43326]\n",
      "loss: 2.167730  [ 7104/43326]\n",
      "loss: 2.172362  [ 7744/43326]\n",
      "loss: 2.153558  [ 8384/43326]\n",
      "loss: 2.170492  [ 9024/43326]\n",
      "loss: 2.153249  [ 9664/43326]\n",
      "loss: 2.153173  [10304/43326]\n",
      "loss: 2.158978  [10944/43326]\n",
      "loss: 2.152077  [11584/43326]\n",
      "loss: 2.137154  [12224/43326]\n",
      "loss: 2.135778  [12864/43326]\n",
      "loss: 2.129297  [13504/43326]\n",
      "loss: 2.148001  [14144/43326]\n",
      "loss: 2.136446  [14784/43326]\n",
      "loss: 2.113287  [15424/43326]\n",
      "loss: 2.131316  [16064/43326]\n",
      "loss: 2.117888  [16704/43326]\n",
      "loss: 2.149534  [17344/43326]\n",
      "loss: 2.142998  [17984/43326]\n",
      "loss: 2.120459  [18624/43326]\n",
      "loss: 2.104902  [19264/43326]\n",
      "loss: 2.126593  [19904/43326]\n",
      "loss: 2.124002  [20544/43326]\n",
      "loss: 2.100072  [21184/43326]\n",
      "loss: 2.124718  [21824/43326]\n",
      "loss: 2.110647  [22464/43326]\n",
      "loss: 2.095869  [23104/43326]\n",
      "loss: 2.103584  [23744/43326]\n",
      "loss: 2.091589  [24384/43326]\n",
      "loss: 2.110736  [25024/43326]\n",
      "loss: 2.099288  [25664/43326]\n",
      "loss: 2.087403  [26304/43326]\n",
      "loss: 2.094045  [26944/43326]\n",
      "loss: 2.095740  [27584/43326]\n",
      "loss: 2.095223  [28224/43326]\n",
      "loss: 2.076915  [28864/43326]\n",
      "loss: 2.058515  [29504/43326]\n",
      "loss: 2.089480  [30144/43326]\n",
      "loss: 2.083526  [30784/43326]\n",
      "loss: 2.078506  [31424/43326]\n",
      "loss: 2.046347  [32064/43326]\n",
      "loss: 2.059255  [32704/43326]\n",
      "loss: 2.076556  [33344/43326]\n",
      "loss: 2.035537  [33984/43326]\n",
      "loss: 2.015892  [34624/43326]\n",
      "loss: 2.032985  [35264/43326]\n",
      "loss: 2.035385  [35904/43326]\n",
      "loss: 2.053472  [36544/43326]\n",
      "loss: 2.004886  [37184/43326]\n",
      "loss: 2.021213  [37824/43326]\n",
      "loss: 2.062476  [38464/43326]\n",
      "loss: 2.019346  [39104/43326]\n",
      "loss: 2.028241  [39744/43326]\n",
      "loss: 2.019631  [40384/43326]\n",
      "loss: 2.028842  [41024/43326]\n",
      "loss: 2.015756  [41664/43326]\n",
      "loss: 2.002919  [42304/43326]\n",
      "loss: 2.029814  [42944/43326]\n",
      "Epoch 2:\n",
      "loss: 2.042880  [   64/43326]\n",
      "loss: 2.001345  [  704/43326]\n",
      "loss: 2.017621  [ 1344/43326]\n",
      "loss: 1.993396  [ 1984/43326]\n",
      "loss: 1.989907  [ 2624/43326]\n",
      "loss: 1.980642  [ 3264/43326]\n",
      "loss: 1.988448  [ 3904/43326]\n",
      "loss: 1.932089  [ 4544/43326]\n",
      "loss: 1.976600  [ 5184/43326]\n",
      "loss: 1.929578  [ 5824/43326]\n",
      "loss: 1.942695  [ 6464/43326]\n",
      "loss: 1.974145  [ 7104/43326]\n",
      "loss: 1.988594  [ 7744/43326]\n",
      "loss: 1.948691  [ 8384/43326]\n",
      "loss: 1.982716  [ 9024/43326]\n",
      "loss: 1.895748  [ 9664/43326]\n",
      "loss: 1.911168  [10304/43326]\n",
      "loss: 1.989837  [10944/43326]\n",
      "loss: 2.008260  [11584/43326]\n",
      "loss: 1.928428  [12224/43326]\n",
      "loss: 1.940081  [12864/43326]\n",
      "loss: 1.932977  [13504/43326]\n",
      "loss: 1.903530  [14144/43326]\n",
      "loss: 1.911250  [14784/43326]\n",
      "loss: 1.909951  [15424/43326]\n",
      "loss: 1.911286  [16064/43326]\n",
      "loss: 1.919175  [16704/43326]\n",
      "loss: 1.885921  [17344/43326]\n",
      "loss: 1.932181  [17984/43326]\n",
      "loss: 1.876542  [18624/43326]\n",
      "loss: 1.880688  [19264/43326]\n",
      "loss: 1.889073  [19904/43326]\n",
      "loss: 1.848958  [20544/43326]\n",
      "loss: 1.803959  [21184/43326]\n",
      "loss: 1.843953  [21824/43326]\n",
      "loss: 1.787769  [22464/43326]\n",
      "loss: 1.834417  [23104/43326]\n",
      "loss: 1.865265  [23744/43326]\n",
      "loss: 1.822980  [24384/43326]\n",
      "loss: 1.833607  [25024/43326]\n",
      "loss: 1.779138  [25664/43326]\n",
      "loss: 1.708541  [26304/43326]\n",
      "loss: 1.744510  [26944/43326]\n",
      "loss: 1.804782  [27584/43326]\n",
      "loss: 1.719783  [28224/43326]\n",
      "loss: 1.848066  [28864/43326]\n",
      "loss: 1.756575  [29504/43326]\n",
      "loss: 1.708335  [30144/43326]\n",
      "loss: 1.710571  [30784/43326]\n",
      "loss: 1.727592  [31424/43326]\n",
      "loss: 1.845368  [32064/43326]\n",
      "loss: 1.700087  [32704/43326]\n",
      "loss: 1.721279  [33344/43326]\n",
      "loss: 1.713399  [33984/43326]\n",
      "loss: 1.743068  [34624/43326]\n",
      "loss: 1.706984  [35264/43326]\n",
      "loss: 1.700368  [35904/43326]\n",
      "loss: 1.631532  [36544/43326]\n",
      "loss: 1.747918  [37184/43326]\n",
      "loss: 1.614738  [37824/43326]\n",
      "loss: 1.679093  [38464/43326]\n",
      "loss: 1.694143  [39104/43326]\n",
      "loss: 1.629539  [39744/43326]\n",
      "loss: 1.666102  [40384/43326]\n",
      "loss: 1.722426  [41024/43326]\n",
      "loss: 1.674491  [41664/43326]\n",
      "loss: 1.677922  [42304/43326]\n",
      "loss: 1.608350  [42944/43326]\n",
      "Epoch 3:\n",
      "loss: 1.564590  [   64/43326]\n",
      "loss: 1.584731  [  704/43326]\n",
      "loss: 1.601948  [ 1344/43326]\n",
      "loss: 1.565721  [ 1984/43326]\n",
      "loss: 1.577888  [ 2624/43326]\n",
      "loss: 1.558365  [ 3264/43326]\n",
      "loss: 1.643563  [ 3904/43326]\n",
      "loss: 1.625927  [ 4544/43326]\n",
      "loss: 1.560507  [ 5184/43326]\n",
      "loss: 1.578520  [ 5824/43326]\n",
      "loss: 1.568855  [ 6464/43326]\n",
      "loss: 1.541932  [ 7104/43326]\n",
      "loss: 1.464378  [ 7744/43326]\n",
      "loss: 1.561340  [ 8384/43326]\n",
      "loss: 1.509261  [ 9024/43326]\n",
      "loss: 1.515198  [ 9664/43326]\n",
      "loss: 1.475700  [10304/43326]\n",
      "loss: 1.475727  [10944/43326]\n",
      "loss: 1.476119  [11584/43326]\n",
      "loss: 1.505669  [12224/43326]\n",
      "loss: 1.474046  [12864/43326]\n",
      "loss: 1.483312  [13504/43326]\n",
      "loss: 1.399801  [14144/43326]\n",
      "loss: 1.435840  [14784/43326]\n",
      "loss: 1.352660  [15424/43326]\n",
      "loss: 1.505851  [16064/43326]\n",
      "loss: 1.496371  [16704/43326]\n",
      "loss: 1.437039  [17344/43326]\n",
      "loss: 1.415319  [17984/43326]\n",
      "loss: 1.210463  [18624/43326]\n",
      "loss: 1.443593  [19264/43326]\n",
      "loss: 1.348320  [19904/43326]\n",
      "loss: 1.307839  [20544/43326]\n",
      "loss: 1.412933  [21184/43326]\n",
      "loss: 1.441607  [21824/43326]\n",
      "loss: 1.411081  [22464/43326]\n",
      "loss: 1.439577  [23104/43326]\n",
      "loss: 1.220813  [23744/43326]\n",
      "loss: 1.369483  [24384/43326]\n",
      "loss: 1.414379  [25024/43326]\n",
      "loss: 1.392003  [25664/43326]\n",
      "loss: 1.459697  [26304/43326]\n",
      "loss: 1.355319  [26944/43326]\n",
      "loss: 1.256745  [27584/43326]\n",
      "loss: 1.124718  [28224/43326]\n",
      "loss: 1.318275  [28864/43326]\n",
      "loss: 1.246440  [29504/43326]\n",
      "loss: 1.303414  [30144/43326]\n",
      "loss: 1.259380  [30784/43326]\n",
      "loss: 1.149567  [31424/43326]\n",
      "loss: 1.239903  [32064/43326]\n",
      "loss: 1.308072  [32704/43326]\n",
      "loss: 1.228441  [33344/43326]\n",
      "loss: 1.244150  [33984/43326]\n",
      "loss: 1.192726  [34624/43326]\n",
      "loss: 1.331716  [35264/43326]\n",
      "loss: 1.201390  [35904/43326]\n",
      "loss: 1.180017  [36544/43326]\n",
      "loss: 1.177260  [37184/43326]\n",
      "loss: 1.110119  [37824/43326]\n",
      "loss: 1.082845  [38464/43326]\n",
      "loss: 1.050800  [39104/43326]\n",
      "loss: 1.201879  [39744/43326]\n",
      "loss: 0.976123  [40384/43326]\n",
      "loss: 1.149802  [41024/43326]\n",
      "loss: 1.172989  [41664/43326]\n",
      "loss: 1.142990  [42304/43326]\n",
      "loss: 1.060537  [42944/43326]\n",
      "Epoch 4:\n",
      "loss: 1.070247  [   64/43326]\n",
      "loss: 1.222636  [  704/43326]\n",
      "loss: 1.023355  [ 1344/43326]\n",
      "loss: 1.090585  [ 1984/43326]\n",
      "loss: 1.121901  [ 2624/43326]\n",
      "loss: 1.013834  [ 3264/43326]\n",
      "loss: 1.100104  [ 3904/43326]\n",
      "loss: 1.091566  [ 4544/43326]\n",
      "loss: 1.120323  [ 5184/43326]\n",
      "loss: 0.991825  [ 5824/43326]\n",
      "loss: 1.129958  [ 6464/43326]\n",
      "loss: 1.012145  [ 7104/43326]\n",
      "loss: 1.002910  [ 7744/43326]\n",
      "loss: 1.043648  [ 8384/43326]\n",
      "loss: 0.927134  [ 9024/43326]\n",
      "loss: 0.979891  [ 9664/43326]\n",
      "loss: 1.069168  [10304/43326]\n",
      "loss: 1.077925  [10944/43326]\n",
      "loss: 1.033298  [11584/43326]\n",
      "loss: 0.960021  [12224/43326]\n",
      "loss: 0.939222  [12864/43326]\n",
      "loss: 1.039386  [13504/43326]\n",
      "loss: 1.007721  [14144/43326]\n",
      "loss: 0.964727  [14784/43326]\n",
      "loss: 1.066858  [15424/43326]\n",
      "loss: 0.792813  [16064/43326]\n",
      "loss: 0.900346  [16704/43326]\n",
      "loss: 0.883991  [17344/43326]\n",
      "loss: 0.922073  [17984/43326]\n",
      "loss: 0.827910  [18624/43326]\n",
      "loss: 0.891857  [19264/43326]\n",
      "loss: 0.927744  [19904/43326]\n",
      "loss: 0.978141  [20544/43326]\n",
      "loss: 0.903435  [21184/43326]\n",
      "loss: 0.911222  [21824/43326]\n",
      "loss: 0.945454  [22464/43326]\n",
      "loss: 0.891260  [23104/43326]\n",
      "loss: 0.822172  [23744/43326]\n",
      "loss: 0.870870  [24384/43326]\n",
      "loss: 1.139695  [25024/43326]\n",
      "loss: 0.826249  [25664/43326]\n",
      "loss: 0.916026  [26304/43326]\n",
      "loss: 0.906310  [26944/43326]\n",
      "loss: 0.793550  [27584/43326]\n",
      "loss: 0.883808  [28224/43326]\n",
      "loss: 0.889276  [28864/43326]\n",
      "loss: 0.930780  [29504/43326]\n",
      "loss: 0.874117  [30144/43326]\n",
      "loss: 0.794479  [30784/43326]\n",
      "loss: 0.811886  [31424/43326]\n",
      "loss: 0.724337  [32064/43326]\n",
      "loss: 1.040226  [32704/43326]\n",
      "loss: 0.786626  [33344/43326]\n",
      "loss: 0.845542  [33984/43326]\n",
      "loss: 0.835976  [34624/43326]\n",
      "loss: 0.718399  [35264/43326]\n",
      "loss: 0.728766  [35904/43326]\n",
      "loss: 0.927522  [36544/43326]\n",
      "loss: 0.812629  [37184/43326]\n",
      "loss: 0.717301  [37824/43326]\n",
      "loss: 0.772052  [38464/43326]\n",
      "loss: 0.734539  [39104/43326]\n",
      "loss: 0.738805  [39744/43326]\n",
      "loss: 0.773456  [40384/43326]\n",
      "loss: 0.913880  [41024/43326]\n",
      "loss: 0.853327  [41664/43326]\n",
      "loss: 0.816562  [42304/43326]\n",
      "loss: 0.688159  [42944/43326]\n",
      "Epoch 5:\n",
      "loss: 0.729146  [   64/43326]\n",
      "loss: 0.742763  [  704/43326]\n",
      "loss: 0.942843  [ 1344/43326]\n",
      "loss: 0.794270  [ 1984/43326]\n",
      "loss: 0.714800  [ 2624/43326]\n",
      "loss: 0.624089  [ 3264/43326]\n",
      "loss: 0.837247  [ 3904/43326]\n",
      "loss: 0.638943  [ 4544/43326]\n",
      "loss: 0.802459  [ 5184/43326]\n",
      "loss: 0.744478  [ 5824/43326]\n",
      "loss: 0.764647  [ 6464/43326]\n",
      "loss: 0.805038  [ 7104/43326]\n",
      "loss: 0.641119  [ 7744/43326]\n",
      "loss: 0.683605  [ 8384/43326]\n",
      "loss: 0.777659  [ 9024/43326]\n",
      "loss: 0.799630  [ 9664/43326]\n",
      "loss: 0.711894  [10304/43326]\n",
      "loss: 0.689299  [10944/43326]\n",
      "loss: 0.642533  [11584/43326]\n",
      "loss: 0.650757  [12224/43326]\n",
      "loss: 0.739075  [12864/43326]\n",
      "loss: 0.694660  [13504/43326]\n",
      "loss: 0.584999  [14144/43326]\n",
      "loss: 0.702932  [14784/43326]\n",
      "loss: 0.650087  [15424/43326]\n",
      "loss: 0.771746  [16064/43326]\n",
      "loss: 0.582107  [16704/43326]\n",
      "loss: 0.623331  [17344/43326]\n",
      "loss: 0.676241  [17984/43326]\n",
      "loss: 0.635028  [18624/43326]\n",
      "loss: 0.663773  [19264/43326]\n",
      "loss: 0.690736  [19904/43326]\n",
      "loss: 0.681354  [20544/43326]\n",
      "loss: 0.729528  [21184/43326]\n",
      "loss: 0.653683  [21824/43326]\n",
      "loss: 0.731183  [22464/43326]\n",
      "loss: 0.730660  [23104/43326]\n",
      "loss: 0.655156  [23744/43326]\n",
      "loss: 0.706384  [24384/43326]\n",
      "loss: 0.654200  [25024/43326]\n",
      "loss: 0.593462  [25664/43326]\n",
      "loss: 0.642945  [26304/43326]\n",
      "loss: 0.656601  [26944/43326]\n",
      "loss: 0.650370  [27584/43326]\n",
      "loss: 0.616450  [28224/43326]\n",
      "loss: 0.630206  [28864/43326]\n",
      "loss: 0.540534  [29504/43326]\n",
      "loss: 0.450227  [30144/43326]\n",
      "loss: 0.646149  [30784/43326]\n",
      "loss: 0.736847  [31424/43326]\n",
      "loss: 0.864844  [32064/43326]\n",
      "loss: 0.623881  [32704/43326]\n",
      "loss: 0.671086  [33344/43326]\n",
      "loss: 0.650049  [33984/43326]\n",
      "loss: 0.564957  [34624/43326]\n",
      "loss: 0.559403  [35264/43326]\n",
      "loss: 0.561531  [35904/43326]\n",
      "loss: 0.510161  [36544/43326]\n",
      "loss: 0.588733  [37184/43326]\n",
      "loss: 0.485197  [37824/43326]\n",
      "loss: 0.731509  [38464/43326]\n",
      "loss: 0.639942  [39104/43326]\n",
      "loss: 0.631271  [39744/43326]\n",
      "loss: 0.634154  [40384/43326]\n",
      "loss: 0.475300  [41024/43326]\n",
      "loss: 0.547961  [41664/43326]\n",
      "loss: 0.561834  [42304/43326]\n",
      "loss: 0.562430  [42944/43326]\n",
      "Epoch 6:\n",
      "loss: 0.501195  [   64/43326]\n",
      "loss: 0.614445  [  704/43326]\n",
      "loss: 0.803313  [ 1344/43326]\n",
      "loss: 0.474430  [ 1984/43326]\n",
      "loss: 0.635758  [ 2624/43326]\n",
      "loss: 0.576256  [ 3264/43326]\n",
      "loss: 0.664074  [ 3904/43326]\n",
      "loss: 0.632190  [ 4544/43326]\n",
      "loss: 0.557965  [ 5184/43326]\n",
      "loss: 0.588032  [ 5824/43326]\n",
      "loss: 0.505608  [ 6464/43326]\n",
      "loss: 0.753920  [ 7104/43326]\n",
      "loss: 0.581432  [ 7744/43326]\n",
      "loss: 0.606289  [ 8384/43326]\n",
      "loss: 0.554074  [ 9024/43326]\n",
      "loss: 0.609158  [ 9664/43326]\n",
      "loss: 0.599590  [10304/43326]\n",
      "loss: 0.572294  [10944/43326]\n",
      "loss: 0.709563  [11584/43326]\n",
      "loss: 0.476667  [12224/43326]\n",
      "loss: 0.611836  [12864/43326]\n",
      "loss: 0.510751  [13504/43326]\n",
      "loss: 0.465460  [14144/43326]\n",
      "loss: 0.565454  [14784/43326]\n",
      "loss: 0.473167  [15424/43326]\n",
      "loss: 0.511803  [16064/43326]\n",
      "loss: 0.545503  [16704/43326]\n",
      "loss: 0.526814  [17344/43326]\n",
      "loss: 0.463508  [17984/43326]\n",
      "loss: 0.488770  [18624/43326]\n",
      "loss: 0.640807  [19264/43326]\n",
      "loss: 0.427311  [19904/43326]\n",
      "loss: 0.666981  [20544/43326]\n",
      "loss: 0.506015  [21184/43326]\n",
      "loss: 0.425136  [21824/43326]\n",
      "loss: 0.418689  [22464/43326]\n",
      "loss: 0.511457  [23104/43326]\n",
      "loss: 0.595806  [23744/43326]\n",
      "loss: 0.518006  [24384/43326]\n",
      "loss: 0.462257  [25024/43326]\n",
      "loss: 0.567253  [25664/43326]\n",
      "loss: 0.489652  [26304/43326]\n",
      "loss: 0.601359  [26944/43326]\n",
      "loss: 0.472663  [27584/43326]\n",
      "loss: 0.674400  [28224/43326]\n",
      "loss: 0.499696  [28864/43326]\n",
      "loss: 0.493130  [29504/43326]\n",
      "loss: 0.652549  [30144/43326]\n",
      "loss: 0.587011  [30784/43326]\n",
      "loss: 0.448573  [31424/43326]\n",
      "loss: 0.684986  [32064/43326]\n",
      "loss: 0.515685  [32704/43326]\n",
      "loss: 0.395169  [33344/43326]\n",
      "loss: 0.380514  [33984/43326]\n",
      "loss: 0.375096  [34624/43326]\n",
      "loss: 0.561099  [35264/43326]\n",
      "loss: 0.608353  [35904/43326]\n",
      "loss: 0.565533  [36544/43326]\n",
      "loss: 0.655657  [37184/43326]\n",
      "loss: 0.501459  [37824/43326]\n",
      "loss: 0.415255  [38464/43326]\n",
      "loss: 0.583609  [39104/43326]\n",
      "loss: 0.566362  [39744/43326]\n",
      "loss: 0.427231  [40384/43326]\n",
      "loss: 0.548152  [41024/43326]\n",
      "loss: 0.602659  [41664/43326]\n",
      "loss: 0.542014  [42304/43326]\n",
      "loss: 0.471436  [42944/43326]\n",
      "Epoch 7:\n",
      "loss: 0.386809  [   64/43326]\n",
      "loss: 0.404702  [  704/43326]\n",
      "loss: 0.477954  [ 1344/43326]\n",
      "loss: 0.552482  [ 1984/43326]\n",
      "loss: 0.571355  [ 2624/43326]\n",
      "loss: 0.460265  [ 3264/43326]\n",
      "loss: 0.380448  [ 3904/43326]\n",
      "loss: 0.405799  [ 4544/43326]\n",
      "loss: 0.524199  [ 5184/43326]\n",
      "loss: 0.661655  [ 5824/43326]\n",
      "loss: 0.397820  [ 6464/43326]\n",
      "loss: 0.499583  [ 7104/43326]\n",
      "loss: 0.525381  [ 7744/43326]\n",
      "loss: 0.594628  [ 8384/43326]\n",
      "loss: 0.550615  [ 9024/43326]\n",
      "loss: 0.707760  [ 9664/43326]\n",
      "loss: 0.403416  [10304/43326]\n",
      "loss: 0.520661  [10944/43326]\n",
      "loss: 0.559708  [11584/43326]\n",
      "loss: 0.494399  [12224/43326]\n",
      "loss: 0.326988  [12864/43326]\n",
      "loss: 0.426252  [13504/43326]\n",
      "loss: 0.373642  [14144/43326]\n",
      "loss: 0.532482  [14784/43326]\n",
      "loss: 0.356393  [15424/43326]\n",
      "loss: 0.534519  [16064/43326]\n",
      "loss: 0.308698  [16704/43326]\n",
      "loss: 0.496281  [17344/43326]\n",
      "loss: 0.463375  [17984/43326]\n",
      "loss: 0.602392  [18624/43326]\n",
      "loss: 0.456935  [19264/43326]\n",
      "loss: 0.475222  [19904/43326]\n",
      "loss: 0.497702  [20544/43326]\n",
      "loss: 0.517805  [21184/43326]\n",
      "loss: 0.475940  [21824/43326]\n",
      "loss: 0.627706  [22464/43326]\n",
      "loss: 0.417272  [23104/43326]\n",
      "loss: 0.402026  [23744/43326]\n",
      "loss: 0.428020  [24384/43326]\n",
      "loss: 0.452731  [25024/43326]\n",
      "loss: 0.391062  [25664/43326]\n",
      "loss: 0.311520  [26304/43326]\n",
      "loss: 0.294893  [26944/43326]\n",
      "loss: 0.355535  [27584/43326]\n",
      "loss: 0.572414  [28224/43326]\n",
      "loss: 0.308656  [28864/43326]\n",
      "loss: 0.397113  [29504/43326]\n",
      "loss: 0.465137  [30144/43326]\n",
      "loss: 0.352768  [30784/43326]\n",
      "loss: 0.425614  [31424/43326]\n",
      "loss: 0.376917  [32064/43326]\n",
      "loss: 0.489475  [32704/43326]\n",
      "loss: 0.450103  [33344/43326]\n",
      "loss: 0.569793  [33984/43326]\n",
      "loss: 0.391897  [34624/43326]\n",
      "loss: 0.431057  [35264/43326]\n",
      "loss: 0.693066  [35904/43326]\n",
      "loss: 0.405285  [36544/43326]\n",
      "loss: 0.449068  [37184/43326]\n",
      "loss: 0.510542  [37824/43326]\n",
      "loss: 0.524435  [38464/43326]\n",
      "loss: 0.410166  [39104/43326]\n",
      "loss: 0.554342  [39744/43326]\n",
      "loss: 0.364876  [40384/43326]\n",
      "loss: 0.539359  [41024/43326]\n",
      "loss: 0.465949  [41664/43326]\n",
      "loss: 0.532692  [42304/43326]\n",
      "loss: 0.348426  [42944/43326]\n",
      "Epoch 8:\n",
      "loss: 0.621288  [   64/43326]\n",
      "loss: 0.376207  [  704/43326]\n",
      "loss: 0.445126  [ 1344/43326]\n",
      "loss: 0.473414  [ 1984/43326]\n",
      "loss: 0.564649  [ 2624/43326]\n",
      "loss: 0.379779  [ 3264/43326]\n",
      "loss: 0.462996  [ 3904/43326]\n",
      "loss: 0.435567  [ 4544/43326]\n",
      "loss: 0.388910  [ 5184/43326]\n",
      "loss: 0.532947  [ 5824/43326]\n",
      "loss: 0.485642  [ 6464/43326]\n",
      "loss: 0.358629  [ 7104/43326]\n",
      "loss: 0.411660  [ 7744/43326]\n",
      "loss: 0.335987  [ 8384/43326]\n",
      "loss: 0.450477  [ 9024/43326]\n",
      "loss: 0.551456  [ 9664/43326]\n",
      "loss: 0.439556  [10304/43326]\n",
      "loss: 0.435071  [10944/43326]\n",
      "loss: 0.388576  [11584/43326]\n",
      "loss: 0.257585  [12224/43326]\n",
      "loss: 0.455405  [12864/43326]\n",
      "loss: 0.598728  [13504/43326]\n",
      "loss: 0.469117  [14144/43326]\n",
      "loss: 0.270886  [14784/43326]\n",
      "loss: 0.330823  [15424/43326]\n",
      "loss: 0.430282  [16064/43326]\n",
      "loss: 0.475276  [16704/43326]\n",
      "loss: 0.375011  [17344/43326]\n",
      "loss: 0.326862  [17984/43326]\n",
      "loss: 0.379814  [18624/43326]\n",
      "loss: 0.295886  [19264/43326]\n",
      "loss: 0.500295  [19904/43326]\n",
      "loss: 0.394178  [20544/43326]\n",
      "loss: 0.362253  [21184/43326]\n",
      "loss: 0.380842  [21824/43326]\n",
      "loss: 0.348234  [22464/43326]\n",
      "loss: 0.518993  [23104/43326]\n",
      "loss: 0.400400  [23744/43326]\n",
      "loss: 0.418924  [24384/43326]\n",
      "loss: 0.220745  [25024/43326]\n",
      "loss: 0.401747  [25664/43326]\n",
      "loss: 0.361087  [26304/43326]\n",
      "loss: 0.469002  [26944/43326]\n",
      "loss: 0.439667  [27584/43326]\n",
      "loss: 0.680854  [28224/43326]\n",
      "loss: 0.407069  [28864/43326]\n",
      "loss: 0.508387  [29504/43326]\n",
      "loss: 0.320178  [30144/43326]\n",
      "loss: 0.567674  [30784/43326]\n",
      "loss: 0.387263  [31424/43326]\n",
      "loss: 0.437030  [32064/43326]\n",
      "loss: 0.438618  [32704/43326]\n",
      "loss: 0.442280  [33344/43326]\n",
      "loss: 0.288202  [33984/43326]\n",
      "loss: 0.307404  [34624/43326]\n",
      "loss: 0.487148  [35264/43326]\n",
      "loss: 0.336202  [35904/43326]\n",
      "loss: 0.377500  [36544/43326]\n",
      "loss: 0.571949  [37184/43326]\n",
      "loss: 0.354548  [37824/43326]\n",
      "loss: 0.426848  [38464/43326]\n",
      "loss: 0.446192  [39104/43326]\n",
      "loss: 0.398571  [39744/43326]\n",
      "loss: 0.594320  [40384/43326]\n",
      "loss: 0.526063  [41024/43326]\n",
      "loss: 0.458839  [41664/43326]\n",
      "loss: 0.390480  [42304/43326]\n",
      "loss: 0.374310  [42944/43326]\n",
      "Epoch 9:\n",
      "loss: 0.330084  [   64/43326]\n",
      "loss: 0.314828  [  704/43326]\n",
      "loss: 0.319136  [ 1344/43326]\n",
      "loss: 0.361423  [ 1984/43326]\n",
      "loss: 0.352512  [ 2624/43326]\n",
      "loss: 0.361679  [ 3264/43326]\n",
      "loss: 0.403167  [ 3904/43326]\n",
      "loss: 0.534453  [ 4544/43326]\n",
      "loss: 0.314806  [ 5184/43326]\n",
      "loss: 0.270518  [ 5824/43326]\n",
      "loss: 0.312390  [ 6464/43326]\n",
      "loss: 0.333169  [ 7104/43326]\n",
      "loss: 0.516160  [ 7744/43326]\n",
      "loss: 0.242980  [ 8384/43326]\n",
      "loss: 0.467326  [ 9024/43326]\n",
      "loss: 0.393303  [ 9664/43326]\n",
      "loss: 0.475560  [10304/43326]\n",
      "loss: 0.380782  [10944/43326]\n",
      "loss: 0.417564  [11584/43326]\n",
      "loss: 0.342828  [12224/43326]\n",
      "loss: 0.606700  [12864/43326]\n",
      "loss: 0.408816  [13504/43326]\n",
      "loss: 0.351191  [14144/43326]\n",
      "loss: 0.376486  [14784/43326]\n",
      "loss: 0.389138  [15424/43326]\n",
      "loss: 0.521207  [16064/43326]\n",
      "loss: 0.462514  [16704/43326]\n",
      "loss: 0.420498  [17344/43326]\n",
      "loss: 0.297315  [17984/43326]\n",
      "loss: 0.400478  [18624/43326]\n",
      "loss: 0.425312  [19264/43326]\n",
      "loss: 0.427927  [19904/43326]\n",
      "loss: 0.416212  [20544/43326]\n",
      "loss: 0.390767  [21184/43326]\n",
      "loss: 0.509838  [21824/43326]\n",
      "loss: 0.435037  [22464/43326]\n",
      "loss: 0.320790  [23104/43326]\n",
      "loss: 0.320131  [23744/43326]\n",
      "loss: 0.380657  [24384/43326]\n",
      "loss: 0.407491  [25024/43326]\n",
      "loss: 0.264620  [25664/43326]\n",
      "loss: 0.557806  [26304/43326]\n",
      "loss: 0.350609  [26944/43326]\n",
      "loss: 0.355499  [27584/43326]\n",
      "loss: 0.356027  [28224/43326]\n",
      "loss: 0.433321  [28864/43326]\n",
      "loss: 0.420156  [29504/43326]\n",
      "loss: 0.671549  [30144/43326]\n",
      "loss: 0.369832  [30784/43326]\n",
      "loss: 0.430112  [31424/43326]\n",
      "loss: 0.586251  [32064/43326]\n",
      "loss: 0.306588  [32704/43326]\n",
      "loss: 0.272262  [33344/43326]\n",
      "loss: 0.384530  [33984/43326]\n",
      "loss: 0.420712  [34624/43326]\n",
      "loss: 0.324316  [35264/43326]\n",
      "loss: 0.307686  [35904/43326]\n",
      "loss: 0.401255  [36544/43326]\n",
      "loss: 0.585423  [37184/43326]\n",
      "loss: 0.278591  [37824/43326]\n",
      "loss: 0.445929  [38464/43326]\n",
      "loss: 0.307478  [39104/43326]\n",
      "loss: 0.400054  [39744/43326]\n",
      "loss: 0.222141  [40384/43326]\n",
      "loss: 0.420566  [41024/43326]\n",
      "loss: 0.238291  [41664/43326]\n",
      "loss: 0.290030  [42304/43326]\n",
      "loss: 0.589926  [42944/43326]\n",
      "Epoch 10:\n",
      "loss: 0.520380  [   64/43326]\n",
      "loss: 0.275646  [  704/43326]\n",
      "loss: 0.418123  [ 1344/43326]\n",
      "loss: 0.413801  [ 1984/43326]\n",
      "loss: 0.352834  [ 2624/43326]\n",
      "loss: 0.249922  [ 3264/43326]\n",
      "loss: 0.377527  [ 3904/43326]\n",
      "loss: 0.321409  [ 4544/43326]\n",
      "loss: 0.219752  [ 5184/43326]\n",
      "loss: 0.374175  [ 5824/43326]\n",
      "loss: 0.249809  [ 6464/43326]\n",
      "loss: 0.296453  [ 7104/43326]\n",
      "loss: 0.350459  [ 7744/43326]\n",
      "loss: 0.456203  [ 8384/43326]\n",
      "loss: 0.253211  [ 9024/43326]\n",
      "loss: 0.255059  [ 9664/43326]\n",
      "loss: 0.457770  [10304/43326]\n",
      "loss: 0.262544  [10944/43326]\n",
      "loss: 0.510765  [11584/43326]\n",
      "loss: 0.306034  [12224/43326]\n",
      "loss: 0.276062  [12864/43326]\n",
      "loss: 0.421706  [13504/43326]\n",
      "loss: 0.355861  [14144/43326]\n",
      "loss: 0.459339  [14784/43326]\n",
      "loss: 0.277791  [15424/43326]\n",
      "loss: 0.272304  [16064/43326]\n",
      "loss: 0.274398  [16704/43326]\n",
      "loss: 0.449161  [17344/43326]\n",
      "loss: 0.297138  [17984/43326]\n",
      "loss: 0.303480  [18624/43326]\n",
      "loss: 0.263474  [19264/43326]\n",
      "loss: 0.286824  [19904/43326]\n",
      "loss: 0.313149  [20544/43326]\n",
      "loss: 0.442705  [21184/43326]\n",
      "loss: 0.465576  [21824/43326]\n",
      "loss: 0.383500  [22464/43326]\n",
      "loss: 0.356754  [23104/43326]\n",
      "loss: 0.389025  [23744/43326]\n",
      "loss: 0.362199  [24384/43326]\n",
      "loss: 0.454638  [25024/43326]\n",
      "loss: 0.370992  [25664/43326]\n",
      "loss: 0.321326  [26304/43326]\n",
      "loss: 0.342653  [26944/43326]\n",
      "loss: 0.530661  [27584/43326]\n",
      "loss: 0.220117  [28224/43326]\n",
      "loss: 0.443686  [28864/43326]\n",
      "loss: 0.418688  [29504/43326]\n",
      "loss: 0.312685  [30144/43326]\n",
      "loss: 0.348314  [30784/43326]\n",
      "loss: 0.314869  [31424/43326]\n",
      "loss: 0.265907  [32064/43326]\n",
      "loss: 0.403062  [32704/43326]\n",
      "loss: 0.292921  [33344/43326]\n",
      "loss: 0.475135  [33984/43326]\n",
      "loss: 0.228043  [34624/43326]\n",
      "loss: 0.333064  [35264/43326]\n",
      "loss: 0.252147  [35904/43326]\n",
      "loss: 0.264054  [36544/43326]\n",
      "loss: 0.307258  [37184/43326]\n",
      "loss: 0.288136  [37824/43326]\n",
      "loss: 0.433237  [38464/43326]\n",
      "loss: 0.292154  [39104/43326]\n",
      "loss: 0.173269  [39744/43326]\n",
      "loss: 0.312898  [40384/43326]\n",
      "loss: 0.200712  [41024/43326]\n",
      "loss: 0.423331  [41664/43326]\n",
      "loss: 0.349875  [42304/43326]\n",
      "loss: 0.435596  [42944/43326]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.377219 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}:\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, False)\n",
    "\n",
    "test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T13:51:16.965453Z",
     "end_time": "2023-05-01T13:53:30.282891Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:53:30.291021Z",
     "end_time": "2023-05-01T13:53:30.322403Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(model.state_dict(), \"model_B.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T13:53:30.314406Z",
     "end_time": "2023-05-01T13:53:34.069439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.378832 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the model\n",
    "model_B = MlpNet()\n",
    "model_B.load_state_dict(torch.load(\"model_B.pth\"))\n",
    "model_B.eval()\n",
    "test_loop(test_dataloader, model_B, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model at path mnist_models/model_0/final.checkpoint which had accuracy 97.72 and at epoch 10\n",
      "Accuracy of the model A is 97.72\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.040533 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPU_USED = -1\n",
    "\n",
    "def get_pretrained_model(path, data_separated=False):\n",
    "    model = MlpNet()\n",
    "\n",
    "\n",
    "    if GPU_USED != -1:\n",
    "        state = torch.load(\n",
    "            path, map_location=(\n",
    "                lambda s, _: torch.serialization.default_restore_location(s, 'cuda:' + str(GPU_USED))\n",
    "            ),)\n",
    "    else:\n",
    "        state = torch.load(\n",
    "            path, map_location=(\n",
    "                lambda s, _: torch.serialization.default_restore_location(s, 'cpu')\n",
    "            ),)\n",
    "\n",
    "    model_state_dict = state['model_state_dict']\n",
    "\n",
    "    if 'test_accuracy' not in state:\n",
    "        state['test_accuracy'] = -1\n",
    "\n",
    "    if 'epoch' not in state:\n",
    "        state['epoch'] = -1\n",
    "\n",
    "    if not data_separated:\n",
    "        print(\"Loading model at path {} which had accuracy {} and at epoch {}\".format(path, state['test_accuracy'],\n",
    "                                                                                  state['epoch']))\n",
    "    else:\n",
    "        print(\"Loading model at path {} which had local accuracy {} and overall accuracy {} for choice {} at epoch {}\".format(path,\n",
    "            state['local_test_accuracy'], state['test_accuracy'], state['choice'], state['epoch']))\n",
    "\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    if GPU_USED != -1:\n",
    "        model = model.cuda(GPU_USED)\n",
    "\n",
    "    if not data_separated:\n",
    "        return model, state['test_accuracy']\n",
    "    else:\n",
    "        return model, state['test_accuracy'], state['local_test_accuracy']\n",
    "\n",
    "import os\n",
    "\n",
    "args = None\n",
    "model_A, accuracy_A = get_pretrained_model(os.path.join('mnist_models', 'model_{}/{}.checkpoint'.format(0, \"final\")))\n",
    "test_loop(test_dataloader, model_A, loss_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T16:22:54.908717Z",
     "end_time": "2023-05-01T16:22:59.096906Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
